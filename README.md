# Facial Expression Recognition
Facial expression recognition is a field that has gained a lot of attention in recent years due to its role in human-robot interaction and biometric security. The most common models used to train expression detection algorithms are support vector machines and convolutional neural networks; the latter of which is usually more accurate. I created a naive convolutional neural network that is able to score an average accuracy of 65% on the FER 2013 dataset. To increase the accuracy of facial expression detection systems, researchers have typically increased the size of the whole dataset used to train the model, or added more images to labels that have underperformed when testing the model. However, there havenâ€™t been many approaches that use ensemble learning to increase the accuracy of models. The aim of this project is to use a stacking ensemble consisting of models that focus on different parts of the face. The reasoning behind this approach is that oftentimes, specific regions of a face express particular emotions better than others. An intuitive example of this are the emotions of happiness and sadness. Here, the mouth plays the largest role in conveying the emotions (although it is worth noting that eyes and facial lines also help express happiness and sadness to a lesser extent). Given this, I placed models that target the top half and the bottom halves of faces into a stacking ensemble in conjunction with the original model that used the entire face to predict emotion. A stacking ensemble is able to use each model as context and then conditionally decide what models to bias when making the final prediction. Using this method yielded a final model with 83% accuracy - a notable jump of 18% from the original 65%. This approach is a novel way to improve accuracy while maintaining the original size of a dataset, and can be applied to solve a variety of problems beyond just facial expression detection.
